{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0846423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import os # Import the os module for path manipulation\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f3b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndovestDKGAnalyzer:\n",
    "    \"\"\"Comprehensive analysis of IndovestDKG dataset characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer by loading the CSV data and converting the 'date' column.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): The full or relative path to the IndovestDKG CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(csv_path)\n",
    "            # --- START OF FIX ---\n",
    "            # Explicitly convert 'subject', 'object', and 'relation' to string type\n",
    "            # This handles cases where some entries might be interpreted as numbers/floats (e.g., due to NaN)\n",
    "            self.df['subject'] = self.df['subject'].astype(str)\n",
    "            self.df['object'] = self.df['object'].astype(str)\n",
    "            self.df['relation'] = self.df['relation'].astype(str)\n",
    "            self.df['subject_type'] = self.df['subject_type'].astype(str)\n",
    "            self.df['object_type'] = self.df['object_type'].astype(str)\n",
    "            # --- END OF FIX ---\n",
    "            self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "            self.prepare_analysis()\n",
    "            print(f\"Successfully loaded data from: {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file was not found at {csv_path}. Please check the path.\")\n",
    "            # You might want to exit or raise an error here, depending on desired behavior\n",
    "            self.df = pd.DataFrame() # Initialize an empty DataFrame to prevent further errors\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during initialization: {e}\")\n",
    "            self.df = pd.DataFrame()\n",
    "\n",
    "    def prepare_analysis(self):\n",
    "        \"\"\"\n",
    "        Prepares data for analysis by creating sets of all entities and relations,\n",
    "        and mapping entities to their types.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Skipping analysis preparation.\")\n",
    "            self.all_entities = set()\n",
    "            self.all_relations = set()\n",
    "            self.entity_types = {}\n",
    "            return\n",
    "\n",
    "        # Create entity mappings - these columns are already forced to str in __init__\n",
    "        self.all_entities = set(self.df['subject'].tolist() + self.df['object'].tolist())\n",
    "        self.all_relations = set(self.df['relation'].tolist())\n",
    "        \n",
    "        # Create type mappings - these columns are already forced to str in __init__\n",
    "        self.entity_types = {}\n",
    "        for _, row in self.df.iterrows():\n",
    "            self.entity_types[row['subject']] = row['subject_type']\n",
    "            self.entity_types[row['object']] = row['object_type']\n",
    "    \n",
    "    def coverage_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes dataset coverage characteristics, including entity, relation,\n",
    "        and temporal coverage, as well as entity type distribution.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing various coverage statistics.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform coverage analysis.\")\n",
    "            return {}\n",
    "\n",
    "        # Entity coverage analysis - columns are already str\n",
    "        entity_freq = Counter(self.df['subject'].tolist() + self.df['object'].tolist())\n",
    "        \n",
    "        # Relation coverage analysis - column is already str\n",
    "        relation_freq = Counter(self.df['relation'].tolist())\n",
    "        \n",
    "        # Temporal coverage\n",
    "        date_range = self.df['date'].max() - self.df['date'].min()\n",
    "        unique_dates = self.df['date'].dt.date.nunique()\n",
    "        \n",
    "        # Entity type distribution - values are already str\n",
    "        type_distribution = Counter(self.entity_types.values())\n",
    "        \n",
    "        coverage_stats = {\n",
    "            'total_entities': len(self.all_entities),\n",
    "            'total_relations': len(self.all_relations),\n",
    "            'total_quadruplets': len(self.df),\n",
    "            'unique_dates': unique_dates,\n",
    "            'temporal_span_days': date_range.days,\n",
    "            'entity_frequency_stats': {\n",
    "                'mean': np.mean(list(entity_freq.values())) if entity_freq else 0,\n",
    "                'std': np.std(list(entity_freq.values())) if entity_freq else 0,\n",
    "                'min': min(entity_freq.values()) if entity_freq else 0,\n",
    "                'max': max(entity_freq.values()) if entity_freq else 0,\n",
    "                'entities_single_occurrence': sum(1 for freq in entity_freq.values() if freq == 1),\n",
    "                'entities_single_occurrence_pct': (sum(1 for freq in entity_freq.values() if freq == 1) / len(entity_freq) * 100) if entity_freq else 0\n",
    "            },\n",
    "            'relation_frequency_stats': {\n",
    "                'mean': np.mean(list(relation_freq.values())) if relation_freq else 0,\n",
    "                'std': np.std(list(relation_freq.values())) if relation_freq else 0,\n",
    "                'min': min(relation_freq.values()) if relation_freq else 0,\n",
    "                'max': max(relation_freq.values()) if relation_freq else 0,\n",
    "                'relations_single_occurrence': sum(1 for freq in relation_freq.values() if freq == 1),\n",
    "                'relations_single_occurrence_pct': (sum(1 for freq in relation_freq.values() if freq == 1) / len(relation_freq) * 100) if relation_freq else 0\n",
    "            },\n",
    "            'entity_type_distribution': dict(type_distribution),\n",
    "            'temporal_density': len(self.df) / unique_dates if unique_dates > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return coverage_stats\n",
    "    \n",
    "    def sparsity_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes dataset sparsity characteristics, including basic sparsity,\n",
    "        entity-relation sparsity, temporal sparsity, and entity co-occurrence.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing various sparsity statistics.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform sparsity analysis.\")\n",
    "            return {}\n",
    "\n",
    "        num_entities = len(self.all_entities)\n",
    "        num_relations = len(self.all_relations)\n",
    "        num_facts = len(self.df)\n",
    "        \n",
    "        # Theoretical maximum facts (each entity can have each relation with each other entity)\n",
    "        # Handle cases where num_entities or num_relations might be zero\n",
    "        max_possible_facts = num_entities * num_relations * num_entities\n",
    "        \n",
    "        # Basic sparsity\n",
    "        basic_sparsity = 1 - (num_facts / max_possible_facts) if max_possible_facts > 0 else 1.0\n",
    "        \n",
    "        # Entity-relation sparsity\n",
    "        entity_relation_pairs = set()\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Columns are already string due to __init__\n",
    "            entity_relation_pairs.add((row['subject'], row['relation']))\n",
    "            entity_relation_pairs.add((row['object'], row['relation']))\n",
    "        \n",
    "        max_entity_relation_pairs = num_entities * num_relations\n",
    "        entity_relation_sparsity = 1 - (len(entity_relation_pairs) / max_entity_relation_pairs) if max_entity_relation_pairs > 0 else 1.0\n",
    "        \n",
    "        # Temporal sparsity\n",
    "        unique_dates = self.df['date'].dt.date.nunique()\n",
    "        date_range = (self.df['date'].max() - self.df['date'].min()).days + 1\n",
    "        temporal_sparsity = 1 - (unique_dates / date_range) if date_range > 0 else 1.0\n",
    "        \n",
    "        # Entity co-occurrence analysis\n",
    "        entity_pairs = set()\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Columns are already string due to __init__, so sorted() should work now\n",
    "            pair = tuple(sorted([row['subject'], row['object']]))\n",
    "            entity_pairs.add(pair)\n",
    "        \n",
    "        max_entity_pairs = (num_entities * (num_entities - 1)) // 2\n",
    "        entity_pair_coverage = len(entity_pairs) / max_entity_pairs if max_entity_pairs > 0 else 0\n",
    "        \n",
    "        sparsity_stats = {\n",
    "            'basic_sparsity': basic_sparsity,\n",
    "            'entity_relation_sparsity': entity_relation_sparsity,\n",
    "            'temporal_sparsity': temporal_sparsity,\n",
    "            'entity_pair_coverage': entity_pair_coverage,\n",
    "            'facts_per_entity': num_facts / num_entities if num_entities > 0 else 0,\n",
    "            'facts_per_relation': num_facts / num_relations if num_relations > 0 else 0,\n",
    "            'facts_per_day': num_facts / unique_dates if unique_dates > 0 else 0,\n",
    "            'avg_relations_per_entity': len(entity_relation_pairs) / num_entities if num_entities > 0 else 0,\n",
    "            'density_ratio': num_facts / max_possible_facts if max_possible_facts > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return sparsity_stats\n",
    "    \n",
    "    def temporal_pattern_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes temporal patterns within the dataset, including daily, weekly,\n",
    "        and monthly distributions, as well as entity and relation temporal spans.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing various temporal pattern statistics.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform temporal pattern analysis.\")\n",
    "            return {}\n",
    "\n",
    "        # Daily distribution\n",
    "        daily_counts = self.df.groupby(self.df['date'].dt.date).size()\n",
    "        \n",
    "        # Weekly patterns\n",
    "        weekly_counts = self.df.groupby(self.df['date'].dt.dayofweek).size()\n",
    "        \n",
    "        # Monthly patterns\n",
    "        monthly_counts = self.df.groupby(self.df['date'].dt.month).size()\n",
    "        \n",
    "        # Entity temporal behavior\n",
    "        entity_temporal_spans = {}\n",
    "        for entity in self.all_entities: # self.all_entities already contains strings due to .astype(str) in __init__\n",
    "            entity_dates = self.df[\n",
    "                (self.df['subject'] == entity) | (self.df['object'] == entity)\n",
    "            ]['date']\n",
    "            \n",
    "            if len(entity_dates) > 1:\n",
    "                span = (entity_dates.max() - entity_dates.min()).days\n",
    "                entity_temporal_spans[entity] = span\n",
    "        \n",
    "        # Relation temporal behavior\n",
    "        relation_temporal_spans = {}\n",
    "        for relation in self.all_relations: # self.all_relations already contains strings\n",
    "            relation_dates = self.df[self.df['relation'] == relation]['date']\n",
    "            \n",
    "            if len(relation_dates) > 1:\n",
    "                span = (relation_dates.max() - relation_dates.min()).days\n",
    "                relation_temporal_spans[relation] = span\n",
    "        \n",
    "        temporal_stats = {\n",
    "            'daily_variance': daily_counts.var() if not daily_counts.empty else 0,\n",
    "            'daily_mean': daily_counts.mean() if not daily_counts.empty else 0,\n",
    "            'days_with_zero_events': (daily_counts == 0).sum() if not daily_counts.empty else 0,\n",
    "            'max_events_per_day': daily_counts.max() if not daily_counts.empty else 0,\n",
    "            'entity_temporal_spans': {\n",
    "                'mean': np.mean(list(entity_temporal_spans.values())) if entity_temporal_spans else 0,\n",
    "                'std': np.std(list(entity_temporal_spans.values())) if entity_temporal_spans else 0,\n",
    "                'entities_with_temporal_span': len(entity_temporal_spans)\n",
    "            },\n",
    "            'relation_temporal_spans': {\n",
    "                'mean': np.mean(list(relation_temporal_spans.values())) if relation_temporal_spans else 0,\n",
    "                'std': np.std(list(relation_temporal_spans.values())) if relation_temporal_spans else 0,\n",
    "                'relations_with_temporal_span': len(relation_temporal_spans)\n",
    "            },\n",
    "            'temporal_concentration': self._calculate_temporal_concentration(daily_counts)\n",
    "        }\n",
    "        \n",
    "        return temporal_stats\n",
    "    \n",
    "    def _calculate_temporal_concentration(self, daily_counts) -> float:\n",
    "        \"\"\"\n",
    "        Calculates how concentrated events are in time (Gini coefficient-like measure).\n",
    "\n",
    "        Args:\n",
    "            daily_counts (pd.Series): A series of daily event counts.\n",
    "\n",
    "        Returns:\n",
    "            float: The temporal concentration score (Gini coefficient).\n",
    "        \"\"\"\n",
    "        if len(daily_counts) == 0 or daily_counts.sum() == 0:\n",
    "            return 0\n",
    "        \n",
    "        sorted_counts = sorted(daily_counts.values)\n",
    "        n = len(sorted_counts)\n",
    "        cumsum = np.cumsum(sorted_counts)\n",
    "        \n",
    "        # Gini coefficient calculation\n",
    "        gini = (2 * np.sum((np.arange(1, n + 1) * sorted_counts))) / (n * cumsum[-1]) - (n + 1) / n\n",
    "        return gini\n",
    "    \n",
    "    def knowledge_graph_structure_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes the structural characteristics of the knowledge graph,\n",
    "        including nodes, edges, connected components, degree distribution,\n",
    "        clustering coefficient, and density.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing various graph structure statistics.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform graph structure analysis.\")\n",
    "            return {}\n",
    "\n",
    "        # Create NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            # Columns are already string due to __init__\n",
    "            G.add_edge(row['subject'], row['object'], relation=row['relation'], date=row['date'])\n",
    "        \n",
    "        # Basic graph metrics\n",
    "        if len(G.nodes()) > 0:\n",
    "            # Connected components\n",
    "            connected_components = list(nx.connected_components(G))\n",
    "            largest_component_size = max(len(cc) for cc in connected_components) if connected_components else 0\n",
    "            \n",
    "            # Degree analysis\n",
    "            degrees = dict(G.degree())\n",
    "            avg_degree = np.mean(list(degrees.values())) if degrees else 0\n",
    "            degree_std = np.std(list(degrees.values())) if degrees else 0\n",
    "            \n",
    "            # Clustering coefficient\n",
    "            clustering_coeff = nx.average_clustering(G) if len(G.nodes()) > 2 else 0\n",
    "            \n",
    "            # Density\n",
    "            density = nx.density(G)\n",
    "            \n",
    "        else:\n",
    "            largest_component_size = 0\n",
    "            avg_degree = 0\n",
    "            degree_std = 0\n",
    "            clustering_coeff = 0\n",
    "            density = 0\n",
    "            connected_components = []\n",
    "        \n",
    "        structure_stats = {\n",
    "            'num_nodes': len(G.nodes()),\n",
    "            'num_edges': len(G.edges()),\n",
    "            'num_connected_components': len(connected_components),\n",
    "            'largest_component_size': largest_component_size,\n",
    "            'largest_component_ratio': largest_component_size / len(G.nodes()) if len(G.nodes()) > 0 else 0,\n",
    "            'average_degree': avg_degree,\n",
    "            'degree_std': degree_std,\n",
    "            'clustering_coefficient': clustering_coeff,\n",
    "            'graph_density': density,\n",
    "            'isolated_nodes': sum(1 for node in G.nodes() if G.degree(node) == 0)\n",
    "        }\n",
    "        \n",
    "        return structure_stats\n",
    "    \n",
    "    def relation_pattern_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes relation patterns and semantic consistency within the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing various relation pattern statistics.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform relation pattern analysis.\")\n",
    "            return {}\n",
    "\n",
    "        # Relation-type compatibility analysis\n",
    "        relation_type_pairs = defaultdict(set)\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Columns are already string due to __init__\n",
    "            relation_type_pairs[row['relation']].add((row['subject_type'], row['object_type']))\n",
    "        \n",
    "        # Semantic consistency metrics\n",
    "        relation_consistency = {}\n",
    "        for relation, type_pairs in relation_type_pairs.items():\n",
    "            consistency_score = 1.0 / len(type_pairs)  # More type pairs = less consistent\n",
    "            relation_consistency[relation] = {\n",
    "                'num_type_combinations': len(type_pairs),\n",
    "                'consistency_score': consistency_score,\n",
    "                'type_pairs': list(type_pairs)\n",
    "            }\n",
    "        \n",
    "        # Most/least consistent relations\n",
    "        most_consistent = min(relation_consistency.keys(), \n",
    "                              key=lambda r: relation_consistency[r]['num_type_combinations']) if relation_consistency else None\n",
    "        least_consistent = max(relation_consistency.keys(), \n",
    "                               key=lambda r: relation_consistency[r]['num_type_combinations']) if relation_consistency else None\n",
    "        \n",
    "        pattern_stats = {\n",
    "            'relation_consistency': relation_consistency,\n",
    "            'most_consistent_relation': most_consistent,\n",
    "            'least_consistent_relation': least_consistent,\n",
    "            'avg_type_combinations_per_relation': np.mean([\n",
    "                info['num_type_combinations'] for info in relation_consistency.values()\n",
    "            ]) if relation_consistency else 0,\n",
    "            'relations_with_single_type_pair': sum(\n",
    "                1 for info in relation_consistency.values() \n",
    "                if info['num_type_combinations'] == 1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return pattern_stats\n",
    "    \n",
    "    def compare_with_benchmarks(self, benchmark_stats: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Compares the current IndovestDKG dataset with provided benchmark statistics\n",
    "        across coverage, sparsity, and structure metrics.\n",
    "\n",
    "        Args:\n",
    "            benchmark_stats (Dict): A dictionary containing benchmark statistics for comparison.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary showing the comparison results.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot perform benchmark comparison.\")\n",
    "            return {}\n",
    "\n",
    "        current_coverage = self.coverage_analysis()\n",
    "        current_sparsity = self.sparsity_analysis()\n",
    "        current_structure = self.knowledge_graph_structure_analysis()\n",
    "        \n",
    "        comparison = {\n",
    "            'coverage_comparison': {},\n",
    "            'sparsity_comparison': {},\n",
    "            'structure_comparison': {}\n",
    "        }\n",
    "        \n",
    "        # Coverage comparison\n",
    "        for metric in ['total_entities', 'total_relations', 'total_quadruplets']:\n",
    "            if metric in benchmark_stats and current_coverage:\n",
    "                ratio = current_coverage[metric] / benchmark_stats[metric] if benchmark_stats[metric] != 0 else float('inf')\n",
    "                comparison['coverage_comparison'][metric] = {\n",
    "                    'indovest_value': current_coverage[metric],\n",
    "                    'benchmark_value': benchmark_stats[metric],\n",
    "                    'ratio': ratio,\n",
    "                    'status': 'higher' if ratio > 1 else 'lower'\n",
    "                }\n",
    "        \n",
    "        # Sparsity comparison\n",
    "        for metric in ['basic_sparsity', 'temporal_sparsity', 'facts_per_entity']:\n",
    "            if metric in benchmark_stats and current_sparsity:\n",
    "                ratio = current_sparsity[metric] / benchmark_stats[metric] if benchmark_stats[metric] != 0 else float('inf')\n",
    "                comparison['sparsity_comparison'][metric] = {\n",
    "                    'indovest_value': current_sparsity[metric],\n",
    "                    'benchmark_value': benchmark_stats[metric],\n",
    "                    'ratio': ratio,\n",
    "                    'status': 'higher' if ratio > 1 else 'lower'\n",
    "                }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def generate_comprehensive_report(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Generates a comprehensive analysis report including coverage, sparsity,\n",
    "        temporal patterns, graph structure, and relation patterns, along with\n",
    "        performance hypotheses and recommendations.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The complete analysis report.\n",
    "        \"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot generate comprehensive report.\")\n",
    "            return {'error': 'DataFrame is empty'}\n",
    "\n",
    "        report = {\n",
    "            'coverage_analysis': self.coverage_analysis(),\n",
    "            'sparsity_analysis': self.sparsity_analysis(),\n",
    "            'temporal_analysis': self.temporal_pattern_analysis(),\n",
    "            'structure_analysis': self.knowledge_graph_structure_analysis(),\n",
    "            'relation_analysis': self.relation_pattern_analysis()\n",
    "        }\n",
    "        \n",
    "        # Add performance hypothesis\n",
    "        report['performance_hypothesis'] = self._generate_performance_hypothesis(report)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _generate_performance_hypothesis(self, analysis_results: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Generates hypotheses about potential poor model performance based on\n",
    "        the dataset analysis results.\n",
    "\n",
    "        Args:\n",
    "            analysis_results (Dict): The full analysis report dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing identified hypotheses, primary issues, and recommendations.\n",
    "        \"\"\"\n",
    "        hypotheses = []\n",
    "        \n",
    "        # Check if analysis results are available before accessing keys\n",
    "        if 'coverage_analysis' in analysis_results:\n",
    "            coverage = analysis_results['coverage_analysis']\n",
    "            if coverage and coverage['entity_frequency_stats']['entities_single_occurrence_pct'] > 50:\n",
    "                hypotheses.append({\n",
    "                    'category': 'Coverage',\n",
    "                    'issue': 'High percentage of entities with single occurrence',\n",
    "                    'impact': 'Limited learning opportunities for entity embeddings',\n",
    "                    'value': f\"{coverage['entity_frequency_stats']['entities_single_occurrence_pct']:.1f}%\",\n",
    "                    'severity': 'High'\n",
    "                })\n",
    "        \n",
    "        if 'sparsity_analysis' in analysis_results:\n",
    "            sparsity = analysis_results['sparsity_analysis']\n",
    "            if sparsity:\n",
    "                if sparsity['basic_sparsity'] > 0.99:\n",
    "                    hypotheses.append({\n",
    "                        'category': 'Sparsity',\n",
    "                        'issue': 'Extremely high sparsity',\n",
    "                        'impact': 'Insufficient training signal for link prediction',\n",
    "                        'value': f\"{sparsity['basic_sparsity']:.6f}\",\n",
    "                        'severity': 'Critical'\n",
    "                    })\n",
    "                \n",
    "                if sparsity['facts_per_entity'] < 2:\n",
    "                    hypotheses.append({\n",
    "                        'category': 'Sparsity',\n",
    "                        'issue': 'Low facts per entity ratio',\n",
    "                        'impact': 'Insufficient entity-level patterns for learning',\n",
    "                        'value': f\"{sparsity['facts_per_entity']:.2f}\",\n",
    "                        'severity': 'High'\n",
    "                    })\n",
    "        \n",
    "        if 'temporal_analysis' in analysis_results:\n",
    "            temporal = analysis_results['temporal_analysis']\n",
    "            if temporal and temporal['temporal_concentration'] > 0.7:\n",
    "                hypotheses.append({\n",
    "                    'category': 'Temporal',\n",
    "                    'issue': 'High temporal concentration',\n",
    "                    'impact': 'Uneven temporal distribution limits temporal learning',\n",
    "                    'value': f\"{temporal['temporal_concentration']:.3f}\",\n",
    "                    'severity': 'Medium'\n",
    "                })\n",
    "        \n",
    "        if 'structure_analysis' in analysis_results:\n",
    "            structure = analysis_results['structure_analysis']\n",
    "            if structure:\n",
    "                if structure['num_nodes'] > 0 and structure['num_connected_components'] > structure['num_nodes'] * 0.1:\n",
    "                    hypotheses.append({\n",
    "                        'category': 'Structure',\n",
    "                        'issue': 'Highly fragmented graph structure',\n",
    "                        'impact': 'Limited global structure learning',\n",
    "                        'value': f\"{structure['num_connected_components']} components\",\n",
    "                        'severity': 'High'\n",
    "                    })\n",
    "                \n",
    "                if structure['graph_density'] < 0.01:\n",
    "                    hypotheses.append({\n",
    "                        'category': 'Structure',\n",
    "                        'issue': 'Very low graph density',\n",
    "                        'impact': 'Sparse connectivity limits message passing effectiveness',\n",
    "                        'value': f\"{structure['graph_density']:.6f}\",\n",
    "                        'severity': 'High'\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'hypotheses': hypotheses,\n",
    "            'primary_issues': [h for h in hypotheses if h['severity'] in ['Critical', 'High']],\n",
    "            'recommendations': self._generate_recommendations(hypotheses)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, hypotheses: List[Dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates recommendations based on identified issues from the hypotheses.\n",
    "\n",
    "        Args:\n",
    "            hypotheses (List[Dict]): A list of identified hypotheses.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of recommended actions.\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        issue_categories = set(h['category'] for h in hypotheses)\n",
    "        \n",
    "        if 'Coverage' in issue_categories:\n",
    "            recommendations.append(\"Consider entity consolidation or filtering to reduce single-occurrence entities\")\n",
    "            recommendations.append(\"Implement entity linking to merge similar entities\")\n",
    "        \n",
    "        if 'Sparsity' in issue_categories:\n",
    "            recommendations.append(\"Add more data sources to increase fact density\")\n",
    "            recommendations.append(\"Consider data augmentation techniques for knowledge graphs\")\n",
    "            recommendations.append(\"Use pre-trained embeddings to handle sparse entities\")\n",
    "        \n",
    "        if 'Temporal' in issue_categories:\n",
    "            recommendations.append(\"Implement temporal smoothing or interpolation\")\n",
    "            recommendations.append(\"Consider sliding window approaches for temporal modeling\")\n",
    "        \n",
    "        if 'Structure' in issue_categories:\n",
    "            recommendations.append(\"Focus on largest connected component for initial experiments\")\n",
    "            recommendations.append(\"Consider graph construction preprocessing to improve connectivity\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Usage example\n",
    "def analyze_indovest_dataset(csv_path: str):\n",
    "    \"\"\"\n",
    "    Performs a complete analysis of the IndovestDKG dataset and prints a report.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the CSV dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The comprehensive analysis report.\n",
    "    \"\"\"\n",
    "    analyzer = IndovestDKGAnalyzer(csv_path)\n",
    "    if analyzer.df.empty: # Check if DataFrame was loaded successfully\n",
    "        return {\"status\": \"Analysis skipped due to file loading error.\"}\n",
    "\n",
    "    report = analyzer.generate_comprehensive_report()\n",
    "    \n",
    "    print(\"=== INDOVESTDKG COMPREHENSIVE ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Coverage Analysis\n",
    "    coverage = report.get('coverage_analysis', {}) # Use .get to safely access keys\n",
    "    print(\"📊 COVERAGE ANALYSIS:\")\n",
    "    print(f\"  • Total Entities: {coverage.get('total_entities', 0):,}\")\n",
    "    print(f\"  • Total Relations: {coverage.get('total_relations', 0):,}\")\n",
    "    print(f\"  • Total Quadruplets: {coverage.get('total_quadruplets', 0):,}\")\n",
    "    print(f\"  • Entities with single occurrence: {coverage.get('entity_frequency_stats', {}).get('entities_single_occurrence_pct', 0):.1f}%\")\n",
    "    print(f\"  • Relations with single occurrence: {coverage.get('relation_frequency_stats', {}).get('relations_single_occurrence_pct', 0):.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Sparsity Analysis\n",
    "    sparsity = report.get('sparsity_analysis', {})\n",
    "    print(\"🕳️ SPARSITY ANALYSIS:\")\n",
    "    print(f\"  • Basic Sparsity: {sparsity.get('basic_sparsity', 1.0):.6f} ({(1-sparsity.get('basic_sparsity', 1.0))*100:.4f}% filled)\")\n",
    "    print(f\"  • Facts per Entity: {sparsity.get('facts_per_entity', 0):.2f}\")\n",
    "    print(f\"  • Facts per Relation: {sparsity.get('facts_per_relation', 0):.2f}\")\n",
    "    print(f\"  • Entity Pair Coverage: {sparsity.get('entity_pair_coverage', 0):.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Structure Analysis\n",
    "    structure = report.get('structure_analysis', {})\n",
    "    print(\"🔗 GRAPH STRUCTURE ANALYSIS:\")\n",
    "    print(f\"  • Graph Density: {structure.get('graph_density', 0):.6f}\")\n",
    "    print(f\"  • Connected Components: {structure.get('num_connected_components', 0)}\")\n",
    "    print(f\"  • Largest Component Ratio: {structure.get('largest_component_ratio', 0):.3f}\")\n",
    "    print(f\"  • Average Degree: {structure.get('average_degree', 0):.2f}\")\n",
    "    print(f\"  • Clustering Coefficient: {structure.get('clustering_coefficient', 0):.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Performance Hypothesis\n",
    "    hypothesis = report.get('performance_hypothesis', {'primary_issues': [], 'recommendations': []})\n",
    "    print(\"🎯 PERFORMANCE HYPOTHESIS:\")\n",
    "    print(f\"  Primary Issues Identified: {len(hypothesis['primary_issues'])}\")\n",
    "    \n",
    "    for issue in hypothesis['primary_issues']:\n",
    "        print(f\"  • {issue['category']}: {issue['issue']}\")\n",
    "        print(f\"    - Impact: {issue['impact']}\")\n",
    "        print(f\"    - Value: {issue['value']}\")\n",
    "        print(f\"    - Severity: {issue['severity']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"💡 RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(hypothesis['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c0fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load CSV from: d:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\dataset\\KOMPAS\\2-FULL\\IndovestDKG_FULL.csv\n",
      "Successfully loaded data from: d:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\dataset\\KOMPAS\\2-FULL\\IndovestDKG_FULL.csv\n",
      "=== INDOVESTDKG COMPREHENSIVE ANALYSIS ===\n",
      "\n",
      "📊 COVERAGE ANALYSIS:\n",
      "  • Total Entities: 53,522\n",
      "  • Total Relations: 3,314\n",
      "  • Total Quadruplets: 69,336\n",
      "  • Entities with single occurrence: 74.0%\n",
      "  • Relations with single occurrence: 58.5%\n",
      "\n",
      "🕳️ SPARSITY ANALYSIS:\n",
      "  • Basic Sparsity: 1.000000 (0.0000% filled)\n",
      "  • Facts per Entity: 1.30\n",
      "  • Facts per Relation: 20.92\n",
      "  • Entity Pair Coverage: 0.000044\n",
      "\n",
      "🔗 GRAPH STRUCTURE ANALYSIS:\n",
      "  • Graph Density: 0.000044\n",
      "  • Connected Components: 5428\n",
      "  • Largest Component Ratio: 0.752\n",
      "  • Average Degree: 2.36\n",
      "  • Clustering Coefficient: 0.0171\n",
      "\n",
      "🎯 PERFORMANCE HYPOTHESIS:\n",
      "  Primary Issues Identified: 5\n",
      "  • Coverage: High percentage of entities with single occurrence\n",
      "    - Impact: Limited learning opportunities for entity embeddings\n",
      "    - Value: 74.0%\n",
      "    - Severity: High\n",
      "\n",
      "  • Sparsity: Extremely high sparsity\n",
      "    - Impact: Insufficient training signal for link prediction\n",
      "    - Value: 1.000000\n",
      "    - Severity: Critical\n",
      "\n",
      "  • Sparsity: Low facts per entity ratio\n",
      "    - Impact: Insufficient entity-level patterns for learning\n",
      "    - Value: 1.30\n",
      "    - Severity: High\n",
      "\n",
      "  • Structure: Highly fragmented graph structure\n",
      "    - Impact: Limited global structure learning\n",
      "    - Value: 5428 components\n",
      "    - Severity: High\n",
      "\n",
      "  • Structure: Very low graph density\n",
      "    - Impact: Sparse connectivity limits message passing effectiveness\n",
      "    - Value: 0.000044\n",
      "    - Severity: High\n",
      "\n",
      "💡 RECOMMENDATIONS:\n",
      "  1. Consider entity consolidation or filtering to reduce single-occurrence entities\n",
      "  2. Implement entity linking to merge similar entities\n",
      "  3. Add more data sources to increase fact density\n",
      "  4. Consider data augmentation techniques for knowledge graphs\n",
      "  5. Use pre-trained embeddings to handle sparse entities\n",
      "  6. Focus on largest connected component for initial experiments\n",
      "  7. Consider graph construction preprocessing to improve connectivity\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get the current working directory. In a Jupyter/IPython notebook,\n",
    "    # this is typically the directory where the notebook was launched from.\n",
    "    current_working_dir = os.getcwd()\n",
    "\n",
    "    # Construct the path to the CSV file relative to the current working directory.\n",
    "    # Assuming your structure is:\n",
    "    # D:\\Yaffa\\IndovestDKG\\\n",
    "    # └── KG_CONSTRUCTION/\n",
    "    #     ├── notebook/\n",
    "    #     │   └── evaluate-construction-llm.ipynb (your current script, and likely CWD)\n",
    "    #     └── data/\n",
    "    #         └── dataset/\n",
    "    #             └── KOMPAS/\n",
    "    #                 └── 2-FULL/\n",
    "    #                     └── IndovestDKG_FULL.csv\n",
    "    \n",
    "    # Go up one directory from 'notebook' (current_working_dir) to 'KG_CONSTRUCTION/',\n",
    "    # then navigate into 'data/dataset/KOMPAS/2-FULL/'\n",
    "    relative_csv_path = os.path.join(\n",
    "        current_working_dir,\n",
    "        '..', # Go up one level from 'notebook' to 'KG_CONSTRUCTION/'\n",
    "        'data', 'dataset', 'KOMPAS', '2-FULL',\n",
    "        'IndovestDKG_FULL.csv'\n",
    "    )\n",
    "\n",
    "    # Normalize the path to handle '..' correctly and get the absolute path\n",
    "    csv_file_path = os.path.abspath(relative_csv_path)\n",
    "\n",
    "    print(f\"Attempting to load CSV from: {csv_file_path}\")\n",
    "    report = analyze_indovest_dataset(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdea3f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pt manulife aset manajemen indonesia</td>\n",
       "      <td>PERUSAHAAN</td>\n",
       "      <td>Mengumumkan</td>\n",
       "      <td>diversifikasi investasi</td>\n",
       "      <td>KONSEP</td>\n",
       "      <td>2025-03-07 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ihsg</td>\n",
       "      <td>INDIKATOR_EKONOMI</td>\n",
       "      <td>Menghasilkan</td>\n",
       "      <td>pertumbuhan tahunan tertinggi</td>\n",
       "      <td>INDIKATOR_EKONOMI</td>\n",
       "      <td>2025-03-07 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strategi diversifikasi</td>\n",
       "      <td>KONSEP</td>\n",
       "      <td>Meningkatkan</td>\n",
       "      <td>peluang return</td>\n",
       "      <td>KONSEP</td>\n",
       "      <td>2025-03-07 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investor</td>\n",
       "      <td>ORANG</td>\n",
       "      <td>Mengendalikan</td>\n",
       "      <td>berbagai jenis aset investasi</td>\n",
       "      <td>KONSEP</td>\n",
       "      <td>2025-03-07 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dimas ardinugraha</td>\n",
       "      <td>ORANG</td>\n",
       "      <td>Mempengaruhi</td>\n",
       "      <td>strategi diversifikasi</td>\n",
       "      <td>KONSEP</td>\n",
       "      <td>2025-03-07 21:18:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                subject       subject_type       relation  \\\n",
       "0  pt manulife aset manajemen indonesia         PERUSAHAAN    Mengumumkan   \n",
       "1                                  ihsg  INDIKATOR_EKONOMI   Menghasilkan   \n",
       "2                strategi diversifikasi             KONSEP   Meningkatkan   \n",
       "3                              investor              ORANG  Mengendalikan   \n",
       "4                     dimas ardinugraha              ORANG   Mempengaruhi   \n",
       "\n",
       "                          object        object_type                 date  \n",
       "0        diversifikasi investasi             KONSEP  2025-03-07 21:18:00  \n",
       "1  pertumbuhan tahunan tertinggi  INDIKATOR_EKONOMI  2025-03-07 21:18:00  \n",
       "2                 peluang return             KONSEP  2025-03-07 21:18:00  \n",
       "3  berbagai jenis aset investasi             KONSEP  2025-03-07 21:18:00  \n",
       "4         strategi diversifikasi             KONSEP  2025-03-07 21:18:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(csv_file_path).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812d93f",
   "metadata": {},
   "source": [
    "### Analisis di ICEWS14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e147d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33883716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_id_to_name_map(filepath: str) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Loads entity or relation ID to name mappings from a text file.\n",
    "    Expected format: <name> <id> (e.g., China 0)\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    name = parts[0].strip()\n",
    "                    id_val = int(parts[1].strip())\n",
    "                    mapping[id_val] = name\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping malformed line in {filepath}: {line.strip()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Mapping file not found at {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {filepath}: {e}\")\n",
    "    return mapping\n",
    "\n",
    "def process_icews_triplets(\n",
    "    filepath: str,\n",
    "    entity_map: Dict[int, str],\n",
    "    relation_map: Dict[int, str],\n",
    "    base_date: datetime\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Processes a single ICEWS triplet file (train, test, or valid)\n",
    "    and converts it into a list of dictionaries with mapped names and dates.\n",
    "    Expected triplet format: head_id relation_id tail_id timestamp_id 0\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5: # Expecting 5 parts: head, relation, tail, timestamp, and the trailing '0'\n",
    "                    try:\n",
    "                        head_id = int(parts[0])\n",
    "                        relation_id = int(parts[1])\n",
    "                        tail_id = int(parts[2])\n",
    "                        timestamp_id = int(parts[3])\n",
    "\n",
    "                        subject_name = entity_map.get(head_id, f\"UNKNOWN_ENTITY_ID_{head_id}\")\n",
    "                        relation_name = relation_map.get(relation_id, f\"UNKNOWN_RELATION_ID_{relation_id}\")\n",
    "                        object_name = entity_map.get(tail_id, f\"UNKNOWN_ENTITY_ID_{tail_id}\")\n",
    "\n",
    "                        # Calculate the actual date from the timestamp_id\n",
    "                        # Assuming timestamp_id 0 corresponds to the base_date\n",
    "                        event_date = base_date + timedelta(days=timestamp_id)\n",
    "\n",
    "                        data.append({\n",
    "                            'subject': subject_name,\n",
    "                            'subject_type': 'ACTOR', # Placeholder as ICEWS doesn't provide explicit types\n",
    "                            'relation': relation_name,\n",
    "                            'object': object_name,\n",
    "                            'object_type': 'ACTOR', # Placeholder\n",
    "                            'date': event_date.strftime('%Y-%m-%d %H:%M:%S') # Format to match your previous CSV\n",
    "                        })\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Warning: Skipping line with invalid ID/timestamp format in {filepath}: {line.strip()} - {ve}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping malformed triplet line in {filepath}: {line.strip()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Triplet file not found at {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {filepath}: {e}\")\n",
    "    return data\n",
    "\n",
    "def convert_icews_to_csv(icews_data_dir: str, output_csv_filename: str = 'ICEWS14_Combined_Dataset.csv', current_working_dir: str = None):\n",
    "    \"\"\"\n",
    "    Converts ICEWS14 text datasets (train, test, valid) into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        icews_data_dir (str): The path to the ICEWS14 data directory.\n",
    "        output_csv_filename (str): The name of the output CSV file.\n",
    "        current_working_dir (str): The current working directory for path calculation.\n",
    "    \"\"\"\n",
    "    print(f\"Starting conversion for ICEWS14 dataset from: {icews_data_dir}\")\n",
    "\n",
    "    # Define paths to mapping files\n",
    "    entity2id_path = os.path.join(icews_data_dir, 'entity2id.txt')\n",
    "    relation2id_path = os.path.join(icews_data_dir, 'relation2id.txt')\n",
    "\n",
    "    # Load ID to name mappings\n",
    "    entity_map = load_id_to_name_map(entity2id_path)\n",
    "    relation_map = load_id_to_name_map(relation2id_path)\n",
    "\n",
    "    if not entity_map or not relation_map:\n",
    "        print(\"Error: Entity or relation mappings could not be loaded. Aborting conversion.\")\n",
    "        return\n",
    "\n",
    "    # Define paths to triplet files\n",
    "    train_path = os.path.join(icews_data_dir, 'train.txt')\n",
    "    test_path = os.path.join(icews_data_dir, 'test.txt')\n",
    "    valid_path = os.path.join(icews_data_dir, 'valid.txt')\n",
    "\n",
    "    # Base date for ICEWS14 timestamps (Day 0)\n",
    "    # ICEWS14 typically starts on January 1, 2014\n",
    "    base_date = datetime(2014, 1, 1, 0, 0, 0)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Processing train.txt...\")\n",
    "    all_data.extend(process_icews_triplets(train_path, entity_map, relation_map, base_date))\n",
    "    print(f\"Processing test.txt...\")\n",
    "    all_data.extend(process_icews_triplets(test_path, entity_map, relation_map, base_date))\n",
    "    print(f\"Processing valid.txt...\")\n",
    "    all_data.extend(process_icews_triplets(valid_path, entity_map, relation_map, base_date))\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data was processed. Output CSV will be empty.\")\n",
    "        return\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Convert 'date' column to datetime objects for sorting\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Sort by date (timestamp)\n",
    "    df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "    # --- START OF NEW OUTPUT PATH LOGIC ---\n",
    "    if current_working_dir is None:\n",
    "        current_working_dir = os.getcwd() # Fallback, should be provided by __main__\n",
    "\n",
    "    # Calculate the path to D:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\benchmark\n",
    "    # Assuming current_working_dir is D:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\notebook\n",
    "    project_kg_construction_dir = os.path.abspath(os.path.join(current_working_dir, '..'))\n",
    "    \n",
    "    benchmark_data_dir = os.path.join(project_kg_construction_dir, 'data', 'benchmark')\n",
    "    dataset_output_dir = os.path.join(benchmark_data_dir, 'ICEWS14') # Create ICEWS14 subfolder\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output CSV path within the new directory\n",
    "    output_csv_path = os.path.join(dataset_output_dir, output_csv_filename)\n",
    "    # --- END OF NEW OUTPUT PATH LOGIC ---\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Successfully converted ICEWS14 data to CSV: {output_csv_path}\")\n",
    "    print(f\"Total quadruplets in CSV: {len(df):,}\")\n",
    "\n",
    "    return df # Return the DataFrame for potential further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3317d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved ICEWS14 data directory: d:\\Yaffa\\IndovestDKG\\DKG\\GNN-based\\EvoKG\\data\\ICEWS14\n",
      "Starting conversion for ICEWS14 dataset from: d:\\Yaffa\\IndovestDKG\\DKG\\GNN-based\\EvoKG\\data\\ICEWS14\n",
      "Processing train.txt...\n",
      "Processing test.txt...\n",
      "Processing valid.txt...\n",
      "Successfully converted ICEWS14 data to CSV: d:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\benchmark\\ICEWS14\\ICEWS14_Combined_Dataset.csv\n",
      "Total quadruplets in CSV: 665,304\n",
      "\n",
      "Sample of the converted DataFrame:\n",
      "             subject subject_type  \\\n",
      "0  Citizen_(Nigeria)        ACTOR   \n",
      "1          Hezbollah        ACTOR   \n",
      "2          Hezbollah        ACTOR   \n",
      "3   Ministry_(Egypt)        ACTOR   \n",
      "4      Media_(India)        ACTOR   \n",
      "\n",
      "                                            relation             object  \\\n",
      "0                              Criticize_or_denounce   Catherine_Ashton   \n",
      "1  Express_intent_to_engage_in_diplomatic_coopera...              Japan   \n",
      "2                            Engage_in_mass_killings  Citizen_(Nigeria)   \n",
      "3                                            Consult      Media_(India)   \n",
      "4                                            Consult   Ministry_(Egypt)   \n",
      "\n",
      "  object_type       date  \n",
      "0       ACTOR 2014-01-01  \n",
      "1       ACTOR 2014-01-01  \n",
      "2       ACTOR 2014-01-01  \n",
      "3       ACTOR 2014-01-01  \n",
      "4       ACTOR 2014-01-01  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665304 entries, 0 to 665303\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   subject       665304 non-null  object        \n",
      " 1   subject_type  665304 non-null  object        \n",
      " 2   relation      665304 non-null  object        \n",
      " 3   object        665304 non-null  object        \n",
      " 4   object_type   665304 non-null  object        \n",
      " 5   date          665304 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 30.5+ MB\n",
      "Alhamdulillah! Konversi berhasil.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # The base directory where your ICEWS14 data is located\n",
    "    # D:\\Yaffa\\IndovestDKG\\DKG\\GNN-based\\EvoKG\\data\\ICEWS14\n",
    "    \n",
    "    # Construct the absolute path to the ICEWS14 data directory\n",
    "    # Assuming this script is run from KG_CONSTRUCTION/notebook/\n",
    "    current_working_dir = os.getcwd()\n",
    "    \n",
    "    icews14_data_base_path = os.path.join(\n",
    "        current_working_dir,\n",
    "        '..', '..', # From notebook/ to IndovestDKG/\n",
    "        'DKG', 'GNN-based', 'EvoKG', 'data', 'ICEWS14'\n",
    "    )\n",
    "    \n",
    "    icews14_data_abs_path = os.path.abspath(icews14_data_base_path)\n",
    "\n",
    "    print(f\"Resolved ICEWS14 data directory: {icews14_data_abs_path}\")\n",
    "\n",
    "    # Perform the conversion, passing the current_working_dir\n",
    "    converted_df = convert_icews_to_csv(icews14_data_abs_path, current_working_dir=current_working_dir)\n",
    "\n",
    "    if converted_df is not None and not converted_df.empty:\n",
    "        print(\"\\nSample of the converted DataFrame:\")\n",
    "        print(converted_df.head())\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        converted_df.info()\n",
    "        print(\"Alhamdulillah! Konversi berhasil.\")\n",
    "    else:\n",
    "        print(\"Konversi gagal atau tidak ada data yang diproses. Haha.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a60bdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load CSV from: d:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\benchmark\\ICEWS14\\ICEWS14_Combined_Dataset.csv\n",
      "Successfully loaded data from: d:\\Yaffa\\IndovestDKG\\KG_CONSTRUCTION\\data\\benchmark\\ICEWS14\\ICEWS14_Combined_Dataset.csv\n",
      "=== INDOVESTDKG COMPREHENSIVE ANALYSIS ===\n",
      "\n",
      "📊 COVERAGE ANALYSIS:\n",
      "  • Total Entities: 12,498\n",
      "  • Total Relations: 260\n",
      "  • Total Quadruplets: 665,304\n",
      "  • Entities with single occurrence: 17.7%\n",
      "  • Relations with single occurrence: 3.8%\n",
      "\n",
      "🕳️ SPARSITY ANALYSIS:\n",
      "  • Basic Sparsity: 0.999984 (0.0016% filled)\n",
      "  • Facts per Entity: 53.23\n",
      "  • Facts per Relation: 2558.86\n",
      "  • Entity Pair Coverage: 0.001534\n",
      "\n",
      "🔗 GRAPH STRUCTURE ANALYSIS:\n",
      "  • Graph Density: 0.001534\n",
      "  • Connected Components: 21\n",
      "  • Largest Component Ratio: 0.997\n",
      "  • Average Degree: 19.17\n",
      "  • Clustering Coefficient: 0.4714\n",
      "\n",
      "🎯 PERFORMANCE HYPOTHESIS:\n",
      "  Primary Issues Identified: 2\n",
      "  • Sparsity: Extremely high sparsity\n",
      "    - Impact: Insufficient training signal for link prediction\n",
      "    - Value: 0.999984\n",
      "    - Severity: Critical\n",
      "\n",
      "  • Structure: Very low graph density\n",
      "    - Impact: Sparse connectivity limits message passing effectiveness\n",
      "    - Value: 0.001534\n",
      "    - Severity: High\n",
      "\n",
      "💡 RECOMMENDATIONS:\n",
      "  1. Add more data sources to increase fact density\n",
      "  2. Consider data augmentation techniques for knowledge graphs\n",
      "  3. Use pre-trained embeddings to handle sparse entities\n",
      "  4. Focus on largest connected component for initial experiments\n",
      "  5. Consider graph construction preprocessing to improve connectivity\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get the current working directory. In a Jupyter/IPython notebook,\n",
    "    # this is typically the directory where the notebook was launched from.\n",
    "    current_working_dir = os.getcwd()\n",
    "\n",
    "    # Construct the path to the CSV file relative to the current working directory.\n",
    "    # Assuming your structure is:\n",
    "    # D:\\Yaffa\\IndovestDKG\\\n",
    "    # └── KG_CONSTRUCTION/\n",
    "    #     ├── notebook/\n",
    "    #     │   └── evaluate-construction-llm.ipynb (your current script, and likely CWD)\n",
    "    #     └── data/\n",
    "    #         └── benchmark/\n",
    "    #             └── ICEWS14/\n",
    "    #                 └── ICEWS14_Combined_Dataset.csv\n",
    "    \n",
    "    # Go up one directory from 'notebook' (current_working_dir) to 'KG_CONSTRUCTION/',\n",
    "    # then navigate into 'data/dataset/KOMPAS/2-FULL/'\n",
    "    relative_csv_path = os.path.join(\n",
    "        current_working_dir,\n",
    "        '..', # Go up one level from 'notebook' to 'KG_CONSTRUCTION/'\n",
    "        'data', 'benchmark', 'ICEWS14',\n",
    "        'ICEWS14_Combined_Dataset.csv'\n",
    "    )\n",
    "\n",
    "    # Normalize the path to handle '..' correctly and get the absolute path\n",
    "    csv_file_path = os.path.abspath(relative_csv_path)\n",
    "\n",
    "    print(f\"Attempting to load CSV from: {csv_file_path}\")\n",
    "    report = analyze_indovest_dataset(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77737cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
